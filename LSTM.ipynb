{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "086cf5b1-b4d8-45de-8ede-9d8d5e101e5b",
      "metadata": {
        "id": "086cf5b1-b4d8-45de-8ede-9d8d5e101e5b"
      },
      "source": [
        "## Copy the output from peersim to convert to .txt file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a6777c-a17c-4cd6-80f0-b93b79d78500",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2a6777c-a17c-4cd6-80f0-b93b79d78500",
        "outputId": "4e851d62-fe7f-45bb-c3d1-c5d24474e0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data written to raw.txt\n"
          ]
        }
      ],
      "source": [
        "# just copy the o/p here\n",
        "data = \"\"\"\n",
        "Current time: 200000\n",
        "Hit Rate Avg: 0.2782738095238094 Network Load Avg: 0.00741199999999995 Messages: 1853.0\n",
        "Current time: 300000\n",
        "Hit Rate Avg: 0.36964697237335875 Network Load Avg: 0.01722399999999993 Messages: 4324.0\n",
        "Current time: 400000\n",
        "Hit Rate Avg: 0.4011483368404285 Network Load Avg: 0.034947999999999924 Messages: 8839.0\n",
        "Current time: 500000\n",
        "Hit Rate Avg: 0.41064700972549867 Network Load Avg: 0.060680000000000005 Messages: 15558.0\n",
        "Current time: 600000\n",
        "Hit Rate Avg: 0.4089157883191851 Network Load Avg: 0.093608 Messages: 24488.0\n",
        "Current time: 700000\n",
        "Hit Rate Avg: 0.411039969436102 Network Load Avg: 0.132196 Messages: 35375.0\n",
        "Current time: 800000\n",
        "Hit Rate Avg: 0.4107319141547009 Network Load Avg: 0.17707599999999996 Messages: 48677.0\n",
        "Current time: 900000\n",
        "Hit Rate Avg: 0.41033795489429886 Network Load Avg: 0.22601199999999994 Messages: 64048.0\n",
        "Current time: 1000000\n",
        "Hit Rate Avg: 0.4104354364195713 Network Load Avg: 0.278332 Messages: 81582.0\n",
        "Current time: 1100000\n",
        "Hit Rate Avg: 0.41135355210320185 Network Load Avg: 0.32891600000000015 Messages: 100002.0\n",
        "Current time: 1200000\n",
        "Hit Rate Avg: 0.41140424416478266 Network Load Avg: 0.3754760000000001 Messages: 118248.0\n",
        "Current time: 1300000\n",
        "Hit Rate Avg: 0.4113160907782131 Network Load Avg: 0.4140280000000002 Messages: 134458.0\n",
        "Current time: 1400000\n",
        "Hit Rate Avg: 0.41191885882745716 Network Load Avg: 0.4451839999999999 Messages: 148353.0\n",
        "Current time: 1500000\n",
        "Hit Rate Avg: 0.4117662629194911 Network Load Avg: 0.47070399999999973 Messages: 160387.0\n",
        "Current time: 1600000\n",
        "Hit Rate Avg: 0.41182815420485314 Network Load Avg: 0.49076399999999937 Messages: 170219.0\n",
        "Current time: 1700000\n",
        "Hit Rate Avg: 0.411925243311749 Network Load Avg: 0.5053440000000002 Messages: 177760.0\n",
        "Current time: 1800000\n",
        "Hit Rate Avg: 0.4122635787987705 Network Load Avg: 0.515452 Messages: 183135.0\n",
        "Current time: 1900000\n",
        "Hit Rate Avg: 0.41216518205939917 Network Load Avg: 0.5215079999999999 Messages: 186335.0\n",
        "Current time: 2000000\n",
        "Hit Rate Avg: 0.412222296854373 Network Load Avg: 0.5234479999999998 Messages: 187432.0\n",
        "Current time: 2100000\n",
        "Hit Rate Avg: 0.412222296854373 Network Load Avg: 0.5234479999999998 Messages: 187432.0\n",
        "Current time: 2200000\n",
        "Hit Rate Avg: 0.412222296854373 Network Load Avg: 0.5234479999999998 Messages: 187432.0\n",
        "Current time: 2300000\n",
        "Hit Rate Avg: 0.412222296854373 Network Load Avg: 0.5234479999999998 Messages: 187432.0\n",
        "Current time: 2400000\n",
        "Hit Rate Avg: 0.412222296854373 Network Load Avg: 0.5234479999999998 Messages: 187432.0\n",
        "\"\"\"\n",
        "\n",
        "# path to ASCII file you want to create from o/p\n",
        "output_file_path = 'raw.txt'\n",
        "\n",
        "with open(output_file_path, 'w') as file:\n",
        "    file.write(data)\n",
        "\n",
        "print(f\"Data written to {output_file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762fccf1-809b-4193-821a-a4d1430cc80f",
      "metadata": {
        "id": "762fccf1-809b-4193-821a-a4d1430cc80f"
      },
      "source": [
        "## Convert historical data to csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2874ec-b4b1-4854-8385-0155ddda5d2f",
      "metadata": {
        "id": "5b2874ec-b4b1-4854-8385-0155ddda5d2f"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def parse_ascii_to_csv(ascii_file_path, csv_file_path):\n",
        "    with open(ascii_file_path, 'r') as file:\n",
        "\n",
        "        extracted_data = []\n",
        "        for line in file:\n",
        "\n",
        "            if line.startswith('Hit Rate'):\n",
        "                data_values = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", line)\n",
        "                extracted_data.append(data_values)\n",
        "\n",
        "        df = pd.DataFrame(extracted_data, columns=['Hit Rate', 'Network Load', 'Num Queries'])\n",
        "\n",
        "        df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "parse_ascii_to_csv('raw.txt', 'parse.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e77af937-5467-4b02-93fc-031d29dcce2c",
      "metadata": {
        "id": "e77af937-5467-4b02-93fc-031d29dcce2c"
      },
      "source": [
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4068e82-9680-4e2f-93e1-a9ae051dd5ba",
      "metadata": {
        "id": "b4068e82-9680-4e2f-93e1-a9ae051dd5ba"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def preprocess_data(input_csv_path):\n",
        "    data = pd.read_csv(input_csv_path)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaled_data = scaler.fit_transform(data[['Hit Rate', 'Network Load', 'Num Queries']])\n",
        "    scaled_df = pd.DataFrame(scaled_data, columns=['Hit Rate', 'Network Load', 'Num Queries'])\n",
        "    return scaled_df, scaler\n",
        "\n",
        "def create_sequences(df, n_input):\n",
        "    X, y = [], []\n",
        "    for i in range(len(df) - n_input):\n",
        "        X.append(df.iloc[i:i+n_input].values)\n",
        "        y.append(df.iloc[i+n_input].values)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def adjust_parameters_v3(metrics, current_params):\n",
        "    q1, q2, low, high = current_params\n",
        "    hit_rate, net_load, num_queries = metrics\n",
        "\n",
        "    # Adjust q1 and q2 based on hit rate and number of queries\n",
        "    if hit_rate > 0.7:  # High success rate\n",
        "        q1 += 10  # Strengthen pheromone increase\n",
        "        q2 += 0.2  # Sharpen pheromone sensitivity\n",
        "    elif hit_rate < 0.2:  # Low success rate\n",
        "        q1 -= 10\n",
        "        q2 -= 0.05\n",
        "\n",
        "    # Modify low and high thresholds based on network load\n",
        "    if net_load > 0.05:\n",
        "        low += 0.05\n",
        "        high -= 0.05\n",
        "    else:\n",
        "        low -= 0.05\n",
        "        high += 0.05\n",
        "\n",
        "    # Normalize values to ensure they remain within acceptable ranges\n",
        "    q1 = max(0, min(100, q1))\n",
        "    q2 = max(0, max(1, q2))\n",
        "    low = max(0, min(1, low))\n",
        "    high = max(0, min(1, high))\n",
        "\n",
        "    return q1, q2, low, high\n",
        "\n",
        "\n",
        "input_csv_path = 'parse.csv'\n",
        "df, scaler = preprocess_data(input_csv_path)\n",
        "\n",
        "n_input = 10  # number of time steps\n",
        "n_features = 3  #Hit Rate, Network Load, Num Queries\n",
        "\n",
        "X, y = create_sequences(df, n_input)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6b8eb4c-7e25-4eb4-b5c6-e88b9959e842",
      "metadata": {
        "id": "f6b8eb4c-7e25-4eb4-b5c6-e88b9959e842"
      },
      "source": [
        "## Run LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ce11ae-9a4b-4a90-b48f-6d7160227844",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3ce11ae-9a4b-4a90-b48f-6d7160227844",
        "outputId": "db291c77-139e-49c2-9e94-d45bae19668f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.8405 - val_loss: 0.8609\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.7982 - val_loss: 0.8154\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.7564 - val_loss: 0.7702\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.7147 - val_loss: 0.7248\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.6730 - val_loss: 0.6784\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.6306 - val_loss: 0.6318\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.5880 - val_loss: 0.5848\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5449 - val_loss: 0.5367\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.5009 - val_loss: 0.4877\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.4562 - val_loss: 0.4374\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4102 - val_loss: 0.3856\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3629 - val_loss: 0.3323\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3144 - val_loss: 0.2774\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2647 - val_loss: 0.2214\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2140 - val_loss: 0.1654\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.1632 - val_loss: 0.1118\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.1146 - val_loss: 0.0647\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0717 - val_loss: 0.0306\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0400 - val_loss: 0.0191\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.0277 - val_loss: 0.0372\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0412 - val_loss: 0.0736\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0708 - val_loss: 0.0981\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0906 - val_loss: 0.0953\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0875 - val_loss: 0.0732\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0682 - val_loss: 0.0460\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 0.0449 - val_loss: 0.0240\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.0263 - val_loss: 0.0112\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0156 - val_loss: 0.0069\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0122 - val_loss: 0.0086\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0138 - val_loss: 0.0132\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.0179 - val_loss: 0.0183\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0223 - val_loss: 0.0223\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0256 - val_loss: 0.0242\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.0269 - val_loss: 0.0241\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0264 - val_loss: 0.0222\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0243 - val_loss: 0.0192\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.0212 - val_loss: 0.0157\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.0176 - val_loss: 0.0125\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.0143 - val_loss: 0.0101\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.0118 - val_loss: 0.0088\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.0102 - val_loss: 0.0082\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0095 - val_loss: 0.0079\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0092 - val_loss: 0.0073\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.0088 - val_loss: 0.0065\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0084 - val_loss: 0.0055\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.0078 - val_loss: 0.0044\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.0073 - val_loss: 0.0035\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.0067 - val_loss: 0.0028\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0063 - val_loss: 0.0024\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.0060 - val_loss: 0.0023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x799229b0a710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Metrics: [[1.049451  1.0290703 1.108806 ]]\n",
            "Adjusted Parameters (q1, q2, low, high): (90, 1.05, 0.1, 0.15000000000000002)\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    LSTM(100, activation='relu', input_shape=(n_input, n_features)),\n",
        "    Dense(n_features)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Predict the latest data\n",
        "latest_data = df.tail(n_input).values.reshape((1, n_input, n_features))\n",
        "predicted_metrics = model.predict(latest_data, verbose=0)\n",
        "print(\"Predicted Metrics:\", predicted_metrics)\n",
        "\n",
        "# Adjust parameters based on predictions\n",
        "adjusted_params = adjust_parameters_v3(predicted_metrics[0], (80, 1, 0.05, 0.2))  # Also add initial parameters\n",
        "print(\"Adjusted Parameters (q1, q2, low, high):\", adjusted_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77984625-0e6e-496a-93f6-976ddbf8e197",
      "metadata": {
        "id": "77984625-0e6e-496a-93f6-976ddbf8e197"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}